services:
  # Main Anything to Everything service
  anything-to-everything:
    # build with:
    # docker buildx build --cache-from=type=registry,ref=localhost:5065/anything-to-everything:cache --cache-to=type=registry,ref=localhost:5065/anything-to-everything:cache,mode=max -t anything-to-everything .
    image: anything-to-everything:latest
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    #   cache_from:
    #     - type=registry,ref=localhost:5065/anything-to-everything:cache
    #   cache_to:
    #     - type=registry,ref=localhost:5065/anything-to-everything:cache
    container_name: anything-to-everything
    ports:
      - "7069:6969"
    volumes:
      # Persist data
      - ./data/work:/app/work
      - ./data/inputs:/app/inputs
      - ./data/outputs:/app/outputs
      - ./data/uploads:/app/uploads
      - ./data/jobs:/app/jobs
      - ./data/checkpoints:/app/checkpoints
      # Mount model cache
      - huggingface-cache:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_URL=http://host.docker.internal:11434
    # depends_on:
    #   - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    # network_mode: host
    networks:
      - everything-network
    # dns:
    #   - 100.100.100.100

  # Ollama service for LLM processing
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: epub-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   restart: unless-stopped
  #   networks:
  #     - epub-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

volumes:
  # ollama-data:
  #   driver: local
  huggingface-cache:
    driver: local

networks:
  everything-network:
    driver: bridge
